<a name="content">目录</a>

[改善深层神经网络](#title)
- [第一周：深度学习的实用层面](#week1)
	- [1.1. 深度学习的训练](#training-of-deeplearning)
	- [1.2. 偏差与方差](#bias-and-variance)
		- [1.2.1. 调整偏差与方差](#adjustment-of-bias-and-variance)
	- [1.3. 正则化](#regularization)
		- [1.3.1. 正则化防止过拟合的原因](#reason-of-regularization-preventing-overfit)
		- [1.3.2. dropout正则化](#dropout-regularization)
		- [1.3.3. 理解dropout](#understand-dropout)
		- [1.3.4. 其他正则化方法](#another-way-to-regularization)
			- [1.3.4.1. 扩增训练数据](#agument-traning-set)
			- [1.3.4.2. Early stopping](#early-stopping)
	- [1.4. 加速训练](#speedup-traning)
		- [1.4.1. 正则化输入](#regularize-input)
		- [1.4.2. 梯度消失/爆炸](#gradient-vanish-or-explosion)
			- [1.4.2.1. 解决方法：权重矩阵初始化](#weight-initialization)
	- [1.5. 梯度检查](#grad-check)
		- [1.5.1. 梯度逼近](#gradient-approximation)
		- [1.5.2. 实施过程](#operating-program)
- [第二周：优化算法](#week2)
	- [2.1. Mini-batch梯度下降方法](#mini-batch-gradient-descent)
		- [2.1.1. 什么是Mini-batch](#what-is-mini-batch)
		- [2.1.2. 理解Mini-batch](#understand-mini-batch)


<h1 name="title">改善深层神经网络</h1>

<p align="center"><img src=/picture/deeplearning.ai.logo.png width="400" /></p>

<a name="week1"><h3>第一周：深度学习的实用层面 [<sup>目录</sup>](#content)</h3></a>

<a name="training-of-deeplearning"><h4>1.1. 深度学习的训练 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-1.jpg width="800" />

<img src=/picture/Improving-DeepNeuralNetwork-week1-2-1.jpg width="800" />

<a name="bias-and-variance"><h4>1.2. 偏差与方差 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-bais-and-variance.png width="800" />

<img src=/picture/Improving-DeepNeuralNetwork-week1-2-2.jpg width="800" />

高偏差与高方差的例子：

<img src=/picture/Improving-DeepNeuralNetwork-2hign-example.png width="800" />

用紫色线画出的分类器具有高偏差和高方差，**高偏差**：它几乎是一条线性分类器，并未拟合数据；**高方差**：曲线中间部分灵活性非常高，却过度拟合了两个错误样本

<a name="adjustment-of-bias-and-variance"><h4>1.2.1. 调整偏差与方差 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-3.jpg width="800" />

<a name="regularization"><h4>1.3. 正则化 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-4.jpg width="800" />

<a name="reason-of-regularization-preventing-overfit"><h4>1.3.1. 正则化防止过拟合的原因 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-5-1.jpg width="800" />

<a name="dropout-regularization"><h4>1.3.2. dropout正则化 [<sup>目录</sup>](#content)</h4></a>

<table>
<tr>
	<td><img src=/picture/Improving-DeepNeuralNetwork-dropout-regularization-1.png width="400" /></td>
	<td><img src=/picture/Improving-DeepNeuralNetwork-dropout-regularization-2.png width="400" /></td>
</tr>
</table>

<img src=/picture/Improving-DeepNeuralNetwork-week1-5-2.jpg width="800" />

<a name="understand-dropout"><h4>1.3.3. 理解dropout [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-6-1.jpg width="800" />

<img src=/picture/Improving-DeepNeuralNetwork-week1-6-2.jpg width="800" />

<a name="another-way-to-regularization"><h4>1.3.4. 其他正则化方法 [<sup>目录</sup>](#content)</h4></a>

<a name="agument-traning-set"><h4>1.3.4.1. 扩增训练数据 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-7-1.jpg width="800" />

<a name="early-stopping"><h4>1.3.4.2. Early stopping [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-7-2.jpg width="800" />

<a name="speedup-traning"><h4>1.4. 加速训练  [<sup>目录</sup>](#content)</h4></a>

<a name="regularize-input"><h4>1.4.1. 加速训练方法一：正则化输入 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-7-3.jpg width="800" />

<table>
<tr>
	<td><img src=/picture/Improving-DeepNeuralNetwork-week2-unnormalized-1.png width="400" /></td>
	<td><img src=/picture/Improving-DeepNeuralNetwork-week2-normalized-1.png width="400" /></td>
</tr>
<tr>
	<td><img src=/picture/Improving-DeepNeuralNetwork-week2-unnormalized-2.png width="400" /></td>
	<td><img src=/picture/Improving-DeepNeuralNetwork-week2-normalized-2.png width="400" /></td>
</tr>
</table>

<img src=/picture/Improving-DeepNeuralNetwork-week1-8-1.jpg width="800" />

<a name="gradient-vanish-or-explosion"><h4>1.4.2. 梯度消失/爆炸 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-8-2.jpg width="800" />

<a name="weight-initialization"><h4>1.4.2.1. 解决方法：权重矩阵初始化 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-9-1.jpg width="800" />

<a name="grad-check"><h4>1.5. 梯度检查 [<sup>目录</sup>](#content)</h4></a>

<a name="gradient-approximation"><h4>1.5.1. 梯度逼近 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-9-2.jpg width="800" />

<a name="operating-program"><h4>1.5.2. 实施过程 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week1-10-1.jpg width="800" />

<img src=/picture/Improving-DeepNeuralNetwork-week1-10-2.jpg width="800" />

<a name="week2"><h3>第二周：优化算法 [<sup>目录</sup>](#content)</h3></a>

<a name="mini-batch-gradient-descent"><h4>2.1. Mini-batch梯度下降方法 [<sup>目录</sup>](#content)</h4></a>

<a name="what-is-mini-batch"><h4>2.1.1. 什么是Mini-batch [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week2-1.jpg width="800" />

<a name="understand-mini-batch"><h4>2.1.2. 理解Mini-batch [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/Improving-DeepNeuralNetwork-week2-2-1.jpg width="800" />

<img src=/picture/Improving-DeepNeuralNetwork-week2-2-2.jpg width="800" />
