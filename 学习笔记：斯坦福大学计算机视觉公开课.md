<a name="content">目录</a>

[学习笔记：斯坦福大学李飞飞机器视觉公开课](#title)
- [CV历史回顾](#history)
- [k近邻与线性分类器](#k-nearest-and-linear-classification)
	- [k近邻](#k-nearest)
		- [超参数与超参数的选择](#hyper-parametric-and-choies)
	- [线性分类器](#linear-classification)
		- [损失函数](#loss-function)
		- [正则化](#regularization)
		- [Softmax Classifier](#softmax)
		- [Softmax vs. SVM](#softmax-vs-svm)
	- [最优化](#optimization)
		- [Mini-batch Gradient Descent](#mini-batch)
- [神经网络初步](#initial-neural-network)
	- [反向传播](#back-propagation)
		- [计算图与计算图导数](#computational-graph-and-derivative)
		- [实例](#bp-example)
		- [深度学习开源框架中的例子](#example-on-opensource-framework)
			- [Torch](#torch)
			- [Caffe](#caffe)
		- [向量化与Jacobian矩阵的稀疏性](#vectorize-and-sparsity-of-jacobian-matrix)
	- [神经网络](#neural-network)
		- [从线性分类器到神经网络](#compare-linear-classifier-and-nn)
		- [训练神经网络](#train-neural-network)
		- [激活函数](#activation-function)
		- [构建神经网络](#form-neural-network)
- [神经网络训练细节](#details-to-train-neural-network)






<h1 name="title">学习笔记：斯坦福大学李飞飞机器视觉公开课</h1>

<a name="history"><h2>CV历史回顾 [<sup>目录</sup>](#content)</h2></a>

<p align="center"><img src=./picture/CV-Note1.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note2.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note3-1.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-ImageNet-error-change.png width=500 /></p>

<p align="center"><img src=./picture/CV-Note3-2.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note4-1.jpg width=800 /></p>

<a name="k-nearest-and-linear-classification"><h2>k近邻与线性分类器 [<sup>目录</sup>](#content)</h2></a>

<p align="center"><img src=./picture/CV-Note4-2.jpg width=800 /></p>

<a name="k-nearest"><h3>k近邻 [<sup>目录</sup>](#content)</h3></a>

显示编程

```
def predict(image):
	# ???
	return class_lable
```

数据驱动

<p align="center"><img src=./picture/CV-data-driven-approach.png width=600 /></p>


<a name="hyper-parametric-and-choies"><h4>超参数与超参数的选择 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-Note5.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-knn-example.png width=800 /></p>


<p align="center"><img src=./picture/CV-Note6-1.jpg width=800 /></p>

<table>
<th>5折验证</th>
<tr>
	<td><p align="center"><img src=./picture/CV-knn-5folds-validation.png width=800 /></p></td>
</tr>
<tr>
	<td><p align="center"><img src=./picture/CV-knn-5folds-cross-validation.png width=800 /></p></td>
</tr>
</table>

<p align="center"><img src=./picture/CV-knn-5folds-cross-validation-accuracy-curve.png width=500 /></p>

<p align="center"><img src=./picture/CV-Note6-2.jpg width=800 /></p>

<a name="linear-classification"><h3>线性分类器 [<sup>目录</sup>](#content)</h3></a>

**应用实例：用神经网络描述图片，CNN+RNN**

<p align="center"><img src=./picture/CV-application-example-cnn-plus-rnn.png width=800 /></p>

- 卷积神经网络：用于计算机视觉
- 循环神经网络：适用于排序问题

<p align="center"><img src=./picture/CV-Note7-1.jpg width=800 /></p>

<a name="loss-function"><h4>损失函数 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-linear-classification-score-example.png width=500 /></p>

<p align="center"><img src=./picture/CV-linear-classification-loss-function.png width=400 /></p>

<p align="center"><img src=./picture/CV-linear-classification-loss-function-usage.png width=500 /></p>

总体损失函数：

<p align="center"><img src=./picture/CV-linear-classification-loss-function-total.png width=300 /></p>

思考：

<p align="center"><img src=./picture/CV-Note7-2.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note8-1.jpg width=800 /></p>

python代码

<p align="center"><img src=./picture/CV-linear-classification-loss-function-code.png width=800 /></p>

但是这个损失函数其实是有问题的！！！

<p align="center"><img src=./picture/CV-linear-classification-loss-function-bug.png width=800 /></p>

当我们在一个数据集中找到一个使损失函数为0的W，那么这个W是**唯一**的吗？

**不唯一**，当找到一个满足使损失函数为0的W时，对其进行线性缩放，可以得到一系列的W空间，并且根据这个损失函数它们的工作方式都相同，它们都满足使损失函数为0

<a name="regularization"><h4>正则化 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-Note9-1.jpg width=800 /></p>

L2 regularization的例子：

<p align="center"><img src=./picture/CV-linear-classification-L2-regularization-example.png width=400 /></p>

<p align="center"><img src=./picture/CV-Note9-2.jpg width=800 /></p>

<a name="softmax"><h4>Softmax Classifier [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-Note10-1.jpg width=800 /></p>

计算损失函数示例：

<p align="center"><img src=./picture/CV-Softmax-classifier-loss-function-usage.png width=800 /></p>

<p align="center"><img src=./picture/CV-Note10-2.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-Note11-1.jpg width=800 /></p>

<a name="softmax-vs-svm"><h4>Softmax vs. SVM [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-Note11-2.jpg width=800 /></p>

<a name="optimization"><h3>最优化 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/CV-Note12-1.jpg width=800 /></p>

数值梯度方法代码：

<p align="center"><img src=./picture/CV-optimization-follow-slope-code.png width=800 /></p>

- approximate
- very slow to evaluate

运算速度慢，且求得的数值并不准确

解决方法：**牛顿-莱布尼茨微积分**

<p align="center"><img src=./picture/CV-Note12-2.jpg width=800 /></p>

梯度下降法代码：

<p align="center"><img src=./picture/CV-optimization-gradient-descent-code.png width=800 /></p>

<p align="center"><img src=./picture/CV-Note13-1.jpg width=800 /></p>

<a name="mini-batch"><h4>Mini-bacth Gradient Descent [<sup>目录</sup>](#content)</h4></a>

only use a small portion of the training set to compute the gradient

代码：

<p align="center" ><img src=./picture/CV-optimization-mini-batch-code.png width=800 /></p>

<p align="center"><img src=./picture/CV-Note13-2.jpg width=800 /></p>

<p align="center"><strong>Loss over mini-batch goes down over time</strong></p>

<p align="center"><img src=./picture/CV-optimization-mini-batch-loss-descent-trend.png width=500 /></p>

<p align="center"><img src=./picture/CV-optimization-effects-of-stepsize.png width=500 /></p>

**建议**：一开始选用一个比较大的学习率，然后随着迭代次数的增加，逐渐将学习率调小

<a name="initial-neural-network"><h2>神经网络初步 [<sup>目录</sup>](#content)</h2></a>

<a name="back-propagation"><h3>反向传播 [<sup>目录</sup>](#content)</h3></a>

<a name="computational-graph-and-derivative"><h4>计算图与计算图导数 [<sup>目录</sup>](#content)</h4></a>

该部分请参考[《学习笔记：神经网络与深度学习》](https://github.com/Ming-Lian/Machine-Learning-Course-in-UCAS/blob/master/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.md#computation-graph-deviation)

<a name="bp-example"><h4>实例 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-bp-example-function-formula.png width=400 /></p>

要用到的导数公式：

<p align="center"><img src=./picture/CV-bp-derivative-formulas.png width=800 /></p>

Back propagation过程：

<p align="center"><img src=./picture/CV-bp-process-example-1.png width=600 /></p>

<p align="center"><img src=./picture/CV-bp-process-example-2.png width=600 /></p>

<p align="center"><img src=./picture/CV-bp-process-example-3.png width=600 /></p>

<p align="center"><img src=./picture/CV-bp-process-example-4.png width=600 /></p>

<p align="center"><img src=./picture/CV-bp-process-example-5.png width=600 /></p>

<p align="center"><img src=./picture/CV-bp-process-example-6.png width=600 /></p>

计算图中不同计算单元在梯度流动中的作用：
- **add gate**: gradient distributor——每个输入分配相同的梯度
- **max gate**: gradient router——较大值的局部梯度为1，较小值局部梯度为0
- **multi gate**: gradient… “switcher”?

<p align="center"><img src=./picture/CV-bp-backward-flow-pattern.png width=600 /></p>

前向/反向传播伪代码：

<p align="center"><img src=./picture/CV-bp-forward-and-backward-psudo-code.png width=800 /></p>

<a name="example-on-opensource-framework"><h4>深度学习开源框架中的例子 [<sup>目录</sup>](#content)</h4></a>

<a name="torch"><h4>Torch [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-Torch-forward.png width=450 /></p>

<p align="center"><img src=./picture/CV-Torch-backward.png width=500 /></p>

<a name="caffe"><h4>Caffe [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-caffe-forward.jpg width=600 /></p>

<p align="center"><img src=./picture/CV-caffe-backward.jpg width=600 /></p>

<a name="vectorize-and-sparsity-of-jacobian-matrix"><h4>向量化与Jacobian矩阵的稀疏性 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-bp-vectorize.png width=600 /></p>

雅克比矩阵（一阶偏导数以一定方式排列成的矩阵）可能存在一定的稀疏性，则可以不需要将整个矩阵求出来，从而提高算法效率

雅克比矩阵稀疏性的例子：

<p align="center"><img src=./picture/CV-bp-vectorize-Jacobian-matrix-sparsity.jpg width=600 /></p>

反向求梯度获得Jacobian matrix，其元素值要么是1，要么是0，则只需要观察那些小于0的输入，然后将这些维度上的梯度置为0

<a name="neural-network"><h3>神经网络 [<sup>目录</sup>](#content)</h3></a>

<a name="compare-linear-classifier-and-nn"><h4>从线性分类器到神经网络 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-compare-linear-classifier-and-nn-1.jpg width=800 /></p>

<p align="center"><img src=./picture/CV-compare-linear-classifier-and-nn-2.jpg width=800 /></p>

<a name="train-neural-network"><h4>训练神经网络 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-neural-network-implementation-of-training-2-layer-nn.png width=800 /></p>

<p align="center"><img src=./picture/CV-neural-network-implementation-of-training-2-layer-nn-code.png width=600 /></p>

<a name="activation-function"><h4>激活函数 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-neural-network-activation-function-1.png width=500 /></p>

<table>
<tr>
	<td><img src=./picture/CV-neural-network-activation-function-2.png width=350 /></td>
	<td><img src=./picture/CV-neural-network-activation-function-3.png width=500 /></td>
</tr>
</table>

历史上，一开始使用的是tanh函数，2012年 **ReLU** 函数开始流行（让神经网络快很多，**推荐作为默认选择**），

<a name="form-neural-network"><h4>构建神经网络 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/CV-neural-network-architecture.png width=800 /></p>

增加隐含层神经元数，带来非线性分类性能的提升

<p align="center"><img src=./picture/CV-neural-network-more-neurons-more-capacity.png width=800 /></p>

加入正则化（对高维度W进行惩罚，防止过拟合）

<p align="center"><img src=./picture/CV-neural-network-affect-of-regularizer.png width=800 /></p>

<a name="details-to-train-neural-network"><h2>神经网络训练细节 [<sup>目录</sup>](#content)</h2></a>

迁移应用过程：

<table>
<tr>
	<td><img src=./picture/CV-transfer-learning-with-cnn-1.png width=250 /></td>
	<td><img src=./picture/CV-transfer-learning-with-cnn-2.png width=350 /></td>
	<td><img src=./picture/CV-transfer-learning-with-cnn-3.png width=400 /></td>
</tr>
</table>



