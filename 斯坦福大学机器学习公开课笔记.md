<a name="content">目录</a>

[斯坦福大学机器学习公开课笔记](#title)
- [符号约定](#sign)
- [线性回归与优化方法](#linner-regression)
	- [目标函数](#object-function)
	- [优化方法一：批量梯度下降](#batch-gradient-descent)
	- [优化方法二：随机梯度下降](#stochastic-gradient-descent)
	- [优化方法三：线性代数方法](#linear-algebra)
	- [参数与非参数学习算法](#parametric-learning-algorithm)
		- [非参数学习算法的例子：Local weighted regression （局部加权回归）](#local-weighted-regression)
	- [目标函数的来源](#origin-of-object-function-linner-regression)
- [分类](#classification)
	- [问题描述与优化方法一：梯度上升](#description)
	- [优化方法二：Newtown's Method](#newtown-method)
	- [目标函数 Logistic function 的来源](#origin-of-object-function-logistic)
		- [指数分布族 (Exponential Family)](#exponential-family)
		- [广义线性模型 (GLM)](#general-linner-model)
- [生成学习算法](#generative-learning-algorithms)
	- [比较：判别学习算法与生产学习算法](#compare)
	- [多元高斯分布](#multi-gaussian-distribution)
	- [高斯判别分析](#gaussian-discriminant-analysis)
	- [高斯判别分析与logistic回归的关系](#relation-between-gda-logistic)

<h1 name="title">斯坦福大学机器学习公开课笔记</h1>

<a name="sign"><h3>符号约定 [<sup>目录</sup>](#content)</h3></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note1.jpg width="800" />

<a name="linner-regression"><h3>线性回归与优化方法 [<sup>目录</sup>](#content)</h3></a>

<a name="object-function"><h4>目标函数</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note2-1.jpg width="800" />

<a name="batch-gradient-descent"><h4>优化方法一：批量梯度下降 (Batch Gradient Descent)</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note2-2.jpg width="800" />

<a name="stochastic-gradient-descent"><h4>优化方法二：随机梯度下降 (Stochastic Gradient Descent)</h4></a>

随机梯度下降方法与批量梯度下降相比，每次迈出一步不一定是下降最快的方向，有时甚至是相反的方向，但是它的总体趋势是下降的。迭代次数多，但是每次迭代所用的时间少，总体效率更高。

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note3-1.jpg width="800" />

<a name="linear-algebra"><h4>优化方法三：线性代数方法</h4></a>

定义矩阵导数，以及矩阵的迹的性质

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note3-2.jpg width="800" />

用线性代数方法求解θ

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note4.jpg width="800" />

最终得到：

<p align="center"> θ = （X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>Y </p>

<a name="parametric-learning-algorithm"><h4>参数与非参数学习算法</h4></a>

- Parametric Learning Algorithm : find set of parametric

- No-parametric Learning Algorithm : no of parametric goes with m

<a name="local-weighted-regression">非参数学习算法的例子：Local weighted regression （局部加权回归）</a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note5-1.jpg width="800" />

<a name="origin-of-object-function-linner-regression"><h4>目标函数的来源</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note6.jpg width="800" />

So maximize L(θ) is the same as minimize

<p align="center"><img src=/picture/Marchine-Learning-Stanford-AndrewNg-equation.png /></p>

<a name="classification"><h3>分类 [<sup>目录</sup>](#content)</h3></a>

<a name="description"><h4>分类问题描述，及用梯度上升方法进行求解</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note7-1.jpg width="800" />

L(θ)的导数推导过程：

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-derivative.PNG width="800" />

<a name="newtown-method"><h4>优化方法二：Newtown's Method</h4></a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note8.jpg width="800" />

<a name="origin-of-object-function-logistic"><h4>目标函数 Logistic function 的来源</h4></a>

<a name="exponential-family">指数分布族 (Exponential Family)</a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note9.jpg width="800" />

<a name="general-linner-model">广义线性模型 (GLM)</a>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note10.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note11.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note12-1.jpg width="800" />

<a name="generative-learning-algorithms"><h3>生成学习算法 [<sup>目录</sup>](#content)</h3></a>

<h4 name="compare">比较：判别学习算法与生产学习算法</h4>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note13-1.jpg width="800" />

<h4 name="multi-gaussian-distribution">多元高斯分布</h4>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note13-2.jpg width="800" />

改变协方差矩阵主对角线上的数值

<table>
<tr>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian1.png width="300" /></td>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian2.png width="300" /></td>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian3.png width="300" /></td>
</tr>
</table>

改变协方差矩阵副对角线上的数值

<table>
<tr>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian4.png width="300" /></td>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian5.png width="300" /></td>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian6.png width="300" /></td>
</tr>
</table>

以等高线图展示

<table>
<tr>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian7.png width="800" /></td>
</tr>
<tr>
	<td><img src=/picture/Marchine-Learning-Stanford-AndrewNg-Gaussian8.png width="800" /></td>
</tr>
</table>

<h4 name="gaussian-discriminant-analysis">高斯判别分析</h4>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note14.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note15-1.jpg width="800" />

<h4 name="relation-between-gda-logistic">高斯判别分析与logistic回归的关系</h4>

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note15-2.jpg width="800" />

<img src=/picture/Marchine-Learning-Stanford-AndrewNg-Note16.jpg width="800" />
